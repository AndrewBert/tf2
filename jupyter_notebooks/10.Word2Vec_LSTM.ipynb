{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/minsuk-heo/tf2/blob/master/jupyter_notebooks/10.Word2Vec_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained Word2Vec\n",
    "embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(df):\n",
    "    \"\"\"\n",
    "    get max token counts from train data, \n",
    "    so we use this number as fixed length input to RNN cell\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "    for row in df['review']:\n",
    "        if len(row.split(\" \")) > max_length:\n",
    "            max_length = len(row.split(\" \"))\n",
    "    return max_length\n",
    "\n",
    "def get_word2vec_enc(reviews):\n",
    "    \"\"\"\n",
    "    get word2vec value for each word in sentence.\n",
    "    concatenate word in numpy array, so we can use it as RNN input\n",
    "    \"\"\"\n",
    "    encoded_reviews = []\n",
    "    for review in reviews:\n",
    "        tokens = review.split(\" \")\n",
    "        review_encoding = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            if i == 0:\n",
    "                sentence_emb = embed([token])\n",
    "            else:\n",
    "                sentence_emb = np.concatenate((sentence_emb, embed([token])), axis=0)\n",
    "        encoded_reviews.append(sentence_emb)\n",
    "    return encoded_reviews\n",
    "        \n",
    "def get_padded_encoded_reviews(encoded_reviews):\n",
    "    \"\"\"\n",
    "    for short sentences, we prepend zero padding so all input to RNN has same length\n",
    "    \"\"\"\n",
    "    padded_reviews_encoding = []\n",
    "    for enc_review in encoded_reviews:\n",
    "        zero_padding_cnt = max_length - enc_review.shape[0]\n",
    "        pad = np.zeros((1, 250))\n",
    "        for i in range(zero_padding_cnt):\n",
    "            enc_review = np.concatenate((pad, enc_review), axis=0)\n",
    "        padded_reviews_encoding.append(enc_review)\n",
    "    return padded_reviews_encoding\n",
    "\n",
    "def sentiment_encode(sentiment):\n",
    "    \"\"\"\n",
    "    return one hot encoding for Y value\n",
    "    \"\"\"\n",
    "    if sentiment == 'positive':\n",
    "        return [1,0]\n",
    "    else:\n",
    "        return [0,1]\n",
    "    \n",
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    encode text value to numeric value\n",
    "    \"\"\"\n",
    "    # encode words into word2vec\n",
    "    reviews = df['review'].tolist()\n",
    "    \n",
    "    encoded_reviews = get_word2vec_enc(reviews)\n",
    "    padded_encoded_reviews = get_padded_encoded_reviews(encoded_reviews)\n",
    "    # encoded sentiment\n",
    "    sentiments = df['sentiment'].tolist()\n",
    "    encoded_sentiment = [sentiment_encode(sentiment) for sentiment in sentiments]\n",
    "    X = np.array(padded_encoded_reviews)\n",
    "    Y = np.array(encoded_sentiment)\n",
    "    return X, Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess (encode text to number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews_train = [\n",
    "         {'review': 'this is the best movie', 'sentiment': 'positive'},\n",
    "         {'review': 'i recommend you watch this movie', 'sentiment': 'positive'},\n",
    "         {'review': 'it was waste of money and time', 'sentiment': 'negative'},\n",
    "         {'review': 'the worst movie ever', 'sentiment': 'negative'}\n",
    "    ]\n",
    "df = pd.DataFrame(movie_reviews_train)\n",
    "\n",
    "# max_length is used for max sequence of input\n",
    "max_length = get_max_length(df)\n",
    "\n",
    "train_X, train_Y = preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 4 samples\n",
      "Epoch 1/50\n",
      "4/4 [==============================] - 3s 868ms/sample - loss: 0.7104 - accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 6ms/sample - loss: 0.6964 - accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 8ms/sample - loss: 0.6828 - accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.6694 - accuracy: 0.7500\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.6563 - accuracy: 0.7500\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.6432 - accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.6303 - accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 9ms/sample - loss: 0.6173 - accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 9ms/sample - loss: 0.6042 - accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 10ms/sample - loss: 0.5910 - accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 9ms/sample - loss: 0.5776 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 6ms/sample - loss: 0.5639 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 9ms/sample - loss: 0.5501 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 8ms/sample - loss: 0.5359 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 8ms/sample - loss: 0.5213 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 8ms/sample - loss: 0.5065 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.4913 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 8ms/sample - loss: 0.4758 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.4599 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 8ms/sample - loss: 0.4437 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 9ms/sample - loss: 0.4272 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 9ms/sample - loss: 0.4103 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 8ms/sample - loss: 0.3933 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 12ms/sample - loss: 0.3759 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 11ms/sample - loss: 0.3584 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 9ms/sample - loss: 0.3408 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 13ms/sample - loss: 0.3230 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 9ms/sample - loss: 0.3052 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 10ms/sample - loss: 0.2874 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 9ms/sample - loss: 0.2697 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 8ms/sample - loss: 0.2520 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.2345 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 8ms/sample - loss: 0.2172 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 11ms/sample - loss: 0.2002 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 10ms/sample - loss: 0.1835 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 11ms/sample - loss: 0.1672 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.1514 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.1361 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.1216 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 6ms/sample - loss: 0.1078 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.0949 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 14ms/sample - loss: 0.0830 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 11ms/sample - loss: 0.0720 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 11ms/sample - loss: 0.0621 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 9ms/sample - loss: 0.0533 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 9ms/sample - loss: 0.0455 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 8ms/sample - loss: 0.0386 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.0328 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 6ms/sample - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 9ms/sample - loss: 0.0235 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x6c251d6d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(train_X, train_Y,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  multiple                  36224     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  66        \n",
      "=================================================================\n",
      "Total params: 36,290\n",
      "Trainable params: 36,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "your model can predict correctly even for unseen words from training.  \n",
    "This is the most benefit of using pretrained word embedding.  \n",
    "Why? pretrained embedding will encode [better], [nice] to similar vector of [best]  \n",
    "even if these words were not in train.  \n",
    "therefore, the input vector to RNN is similar, so correct answers for even these unseen words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/1 - 1s - loss: 0.1969 - accuracy: 1.0000\n",
      "Test score: 0.19689838588237762\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "movie_reviews_train = [\n",
    "         {'review': 'this is the best movie', 'sentiment': 'positive'},\n",
    "         {'review': 'i recommend you watch this movie', 'sentiment': 'positive'},\n",
    "         {'review': 'it was waste of money and time', 'sentiment': 'negative'},\n",
    "         {'review': 'the worst movie ever', 'sentiment': 'negative'}\n",
    "    ]\n",
    "\"\"\"\n",
    "movie_reviews_test = [\n",
    "         {'review': 'it is better movie', 'sentiment': 'positive'},\n",
    "         {'review': 'i suggest you see this movie', 'sentiment': 'positive'},\n",
    "         {'review': 'it was just throwing 20 dollars away', 'sentiment': 'negative'},\n",
    "         {'review': 'worse than any show', 'sentiment': 'negative'},\n",
    "         {'review': 'nice movie, so love it', 'sentiment': 'positive'},\n",
    "         {'review': 'terrible', 'sentiment': 'negative'}\n",
    "    ]\n",
    "test_df = pd.DataFrame(movie_reviews_test)\n",
    "\n",
    "test_X, test_Y = preprocess(test_df)\n",
    "\n",
    "score, acc = model.evaluate(test_X, test_Y, verbose=2)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
